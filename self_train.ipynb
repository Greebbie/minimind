{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b143536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9beda6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e048da3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.3.0+cu121\n",
      "cuda runtime in wheel: 12.1\n",
      "cuda available: True\n",
      "num devices: 1\n",
      "device name: NVIDIA GeForce RTX 4070 Ti\n",
      "platform: Windows-10-10.0.26100-SP0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch, platform\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"cuda runtime in wheel:\", torch.version.cuda)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"num devices:\", torch.cuda.device_count())\n",
    "print(\"device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\")\n",
    "print(\"platform:\", platform.platform())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0d19c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lvbab\\anaconda3\\envs\\mini310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# __package__ = \"trainer\"\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import warnings\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch import optim, nn\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from contextlib import nullcontext\n",
    "from transformers import AutoTokenizer\n",
    "from model.model_minimind import MiniMindConfig, MiniMindForCausalLM\n",
    "from dataset.lm_dataset import PretrainDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d28663",
   "metadata": {},
   "source": [
    "#Pretrain\n",
    "# py 3.10\n",
    "# cuda : pip install torch==2.3.0 --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ed4bdc",
   "metadata": {},
   "source": [
    "env path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1ee39ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: C:\\Users\\lvbab\\Desktop\\minitest\n",
      "Python: c:\\Users\\lvbab\\anaconda3\\envs\\mini310\\python.exe\n",
      "Torch: 2.3.0+cu121 CUDA in wheel: 12.1\n",
      "CUDA available: True Count: 1\n",
      "Device name: NVIDIA GeForce RTX 4070 Ti\n"
     ]
    }
   ],
   "source": [
    "import os, sys, platform, torch\n",
    "from pathlib import Path\n",
    "project_root = Path.cwd().resolve()  \n",
    "sys.path.append(str(project_root))\n",
    "print(\"Project root:\", project_root)\n",
    "print(\"Python:\", sys.executable)\n",
    "print(\"Torch:\", torch.__version__, \"CUDA in wheel:\", torch.version.cuda)\n",
    "print(\"CUDA available:\", torch.cuda.is_available(), \"Count:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cac61d",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd482ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time, math, warnings\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch import optim, nn\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from contextlib import nullcontext\n",
    "from transformers import AutoTokenizer\n",
    "from model.model_minimind import MiniMindConfig, MiniMindForCausalLM\n",
    "from dataset.lm_dataset import PretrainDataset\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def Logger(content, ddp=False):\n",
    "    if not ddp or dist.get_rank() == 0:\n",
    "        print(content)\n",
    "\n",
    "\n",
    "def get_lr(current_step, total_steps, lr):\n",
    "    return lr / 10 + 0.5 * lr * (1 + math.cos(math.pi * current_step / total_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ab6b1a",
   "metadata": {},
   "source": [
    "Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98182a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from types import SimpleNamespace\n",
    "from pathlib import Path\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    out_dir = \"./out\",\n",
    "    epochs = 1,                    # 默认 1，教学演示先从 1 开始\n",
    "    batch_size = 32,\n",
    "    learning_rate = 5e-4,\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
    "    dtype = \"bfloat16\",            # 若遇到问题可改 \"float16\"\n",
    "    use_wandb = False,\n",
    "    wandb_project = \"MiniMind-Pretrain\",\n",
    "    num_workers = 1,\n",
    "    ddp = False,                   # 一般单卡，先 False\n",
    "    accumulation_steps = 8,\n",
    "    grad_clip = 1.0,\n",
    "    warmup_iters = 0,\n",
    "    log_interval = 100,\n",
    "    save_interval = 100,            \n",
    "    hidden_size = 512,\n",
    "    num_hidden_layers = 8,\n",
    "    max_seq_len = 512,\n",
    "    use_moe = False,\n",
    "    data_path = \"./dataset/pretrain_hq.jsonl\",   # 修改为你的数据\n",
    ")\n",
    "\n",
    "# 创建输出目录\n",
    "Path(args.out_dir).mkdir(parents=True, exist_ok=True)\n",
    "device_type = \"cuda\" if \"cuda\" in args.device else \"cpu\"\n",
    "from contextlib import nullcontext\n",
    "ctx = nullcontext() if device_type == \"cpu\" else torch.cuda.amp.autocast()\n",
    "Logger(f\"Using device: {args.device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcfc774",
   "metadata": {},
   "source": [
    "Tokenizer and init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eeced63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params：25.830 M\n"
     ]
    }
   ],
   "source": [
    "tokenizer_path = str(project_root / \"model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "lm_config = MiniMindConfig(hidden_size=args.hidden_size, num_hidden_layers=args.num_hidden_layers, use_moe=args.use_moe)\n",
    "model = MiniMindForCausalLM(lm_config).to(args.device)\n",
    "\n",
    "trainable_params_m = sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6\n",
    "Logger(f\"Total Params：{trainable_params_m:.3f} M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c04dc1",
   "metadata": {},
   "source": [
    "Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e2a006c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_per_epoch = 44160\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = str((project_root / args.data_path).resolve()) if not Path(args.data_path).exists() else args.data_path\n",
    "assert Path(data_path).exists(), f\"数据文件不存在：{data_path}\"\n",
    "train_ds = PretrainDataset(data_path, tokenizer, max_length=args.max_seq_len)\n",
    "train_sampler = DistributedSampler(train_ds) if args.ddp else None\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=args.batch_size,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=args.num_workers,\n",
    "    sampler=train_sampler\n",
    ")\n",
    "iter_per_epoch = len(train_loader)\n",
    "Logger(f\"iter_per_epoch = {iter_per_epoch}\")\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(args.dtype in ['float16', 'bfloat16']))\n",
    "optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dc6f77",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4649cbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ckpt(epoch, print_msg=False):\n",
    "    model.eval()\n",
    "    moe_path = '_moe' if args.use_moe else ''\n",
    "    fn = f'pretrain_{args.hidden_size}{moe_path}_{epoch}.pth'\n",
    "    ckp = str(Path(args.out_dir) / fn)\n",
    "    state_dict = model.state_dict()\n",
    "    state_dict = {k: v.half() for k, v in state_dict.items()}\n",
    "    torch.save(state_dict, ckp)\n",
    "    if print_msg:\n",
    "        Logger(f\"Saved: {ckp}\")\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5a4390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_epoch(epoch, wandb=None, ddp=False):\n",
    "    loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    for step, (X, Y, loss_mask) in enumerate(train_loader):\n",
    "        X = X.to(args.device); Y = Y.to(args.device); loss_mask = loss_mask.to(args.device)\n",
    "\n",
    "        lr = get_lr(epoch * iter_per_epoch + step, args.epochs * iter_per_epoch, args.learning_rate)\n",
    "        for param_group in optimizer.param_groups: param_group['lr'] = lr\n",
    "\n",
    "        with ctx:\n",
    "            res = model(X)\n",
    "            loss = loss_fct(res.logits.view(-1, res.logits.size(-1)), Y.view(-1)).view(Y.size())\n",
    "            loss = (loss * loss_mask).sum() / loss_mask.sum()\n",
    "            loss = loss + getattr(res, \"aux_loss\", 0.0)\n",
    "            loss = loss / args.accumulation_steps\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % args.accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
    "            scaler.step(optimizer); scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if step % args.log_interval == 0:\n",
    "            spend_time = time.time() - start_time\n",
    "            eta_min = int(spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60)\n",
    "            Logger(f\"Epoch:[{epoch+1}/{args.epochs}]({step}/{iter_per_epoch}) \"\n",
    "                   f\"loss:{loss.item()*args.accumulation_steps:.3f} lr:{optimizer.param_groups[-1]['lr']:.12f} \"\n",
    "                   f\"epoch_Time:{eta_min}min:\")\n",
    "\n",
    "        if (step + 1) % args.save_interval == 0:\n",
    "            save_ckpt(epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cfb8bfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[1/1](0/44160) loss:8.954 lr:0.000550000000 epoch_Time:4367min:\n",
      "Epoch:[1/1](100/44160) loss:6.462 lr:0.000549993674 epoch_Time:183min:\n",
      "Epoch:[1/1](200/44160) loss:6.318 lr:0.000549974695 epoch_Time:150min:\n",
      "Epoch:[1/1](300/44160) loss:6.848 lr:0.000549943065 epoch_Time:149min:\n",
      "Epoch:[1/1](400/44160) loss:5.910 lr:0.000549898786 epoch_Time:151min:\n",
      "Epoch:[1/1](500/44160) loss:6.166 lr:0.000549841859 epoch_Time:150min:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m save_ckpt(epoch, print_msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, tag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[30], line 23\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(epoch, wandb, ddp)\u001b[0m\n\u001b[0;32m     21\u001b[0m     scaler\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[0;32m     22\u001b[0m     nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), args\u001b[38;5;241m.\u001b[39mgrad_clip)\n\u001b[1;32m---> 23\u001b[0m     \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m; scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m     24\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m args\u001b[38;5;241m.\u001b[39mlog_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\lvbab\\anaconda3\\envs\\mini310\\lib\\site-packages\\torch\\amp\\grad_scaler.py:453\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[1;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    450\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    451\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 453\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    455\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Users\\lvbab\\anaconda3\\envs\\mini310\\lib\\site-packages\\torch\\amp\\grad_scaler.py:350\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[1;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    344\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    348\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    349\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf_per_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    351\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Users\\lvbab\\anaconda3\\envs\\mini310\\lib\\site-packages\\torch\\amp\\grad_scaler.py:350\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    344\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    348\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    349\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    351\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train_epoch(epoch, wandb=None, ddp=False)\n",
    "save_ckpt(epoch, print_msg=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mini310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
